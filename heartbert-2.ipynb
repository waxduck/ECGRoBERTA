{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[],"toc_visible":true},"gpuClass":"standard","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12111406,"sourceType":"datasetVersion","datasetId":7267352}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # Mounting drive for running the code through Google colab \n# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"lMZC2Cxd79L0","outputId":"7ea27ba9-72f8-426b-db12-80339e476126"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The Python version we use: 3.7.13**","metadata":{"id":"3Xcicu_phAXv"}},{"cell_type":"markdown","source":"# 1- Tokenizer","metadata":{"id":"1divkIWYwSR2"}},{"cell_type":"code","source":"# # We won't need TensorFlow here\n# !pip uninstall -y tensorflow\n\n# # Install `transformers` from master\n# # Install huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.22.0.dev0\n# !pip install git+https://github.com/huggingface/transformers\n# !pip list | grep -E 'transformers|tokenizers'","metadata":{"id":"5duRggBRZKvP","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:43:01.697640Z","iopub.execute_input":"2025-05-30T09:43:01.697889Z","iopub.status.idle":"2025-05-30T09:43:01.707153Z","shell.execute_reply.started":"2025-05-30T09:43:01.697864Z","shell.execute_reply":"2025-05-30T09:43:01.702899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tokenizers import SentencePieceBPETokenizer\n\ntokenizer = SentencePieceBPETokenizer()","metadata":{"id":"xG-80mengvxG","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T14:51:37.574181Z","iopub.execute_input":"2025-06-02T14:51:37.574494Z","iopub.status.idle":"2025-06-02T14:51:37.630361Z","shell.execute_reply.started":"2025-06-02T14:51:37.574456Z","shell.execute_reply":"2025-06-02T14:51:37.629874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"paths = '/kaggle/input/ecgbert/250 Hz/ecg_train.csv' ###path to the train_dataset\nspecial_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<cls>\", \"<sep>\", \"<mask>\"]","metadata":{"id":"Kl_AJAAI4V11","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T14:51:37.630840Z","iopub.execute_input":"2025-06-02T14:51:37.631004Z","iopub.status.idle":"2025-06-02T14:51:37.634483Z","shell.execute_reply.started":"2025-06-02T14:51:37.630990Z","shell.execute_reply":"2025-06-02T14:51:37.633787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.train(\n    paths,\n    vocab_size=52_000,\n    min_frequency=2, #The minimum frequency a pair should have in order to be merged.\n    show_progress=True,\n    limit_alphabet=100, #The maximum different characters to keep in the alphabet.\n    special_tokens=special_tokens\n)\nprint(\"voc size\", tokenizer.get_vocab_size())","metadata":{"id":"Hum6Cw3og_Jz","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T14:51:37.635640Z","iopub.execute_input":"2025-06-02T14:51:37.635819Z","iopub.status.idle":"2025-06-02T14:51:59.758537Z","shell.execute_reply.started":"2025-06-02T14:51:37.635804Z","shell.execute_reply":"2025-06-02T14:51:59.757763Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Making Our Tokenizer ","metadata":{"id":"QqgEAf33uoFH"}},{"cell_type":"code","source":"address = \"tokenizer_pretrained\" #Replace your local address here","metadata":{"id":"8c8ZoiKaoddw","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:18:43.405200Z","iopub.execute_input":"2025-05-30T10:18:43.405434Z","iopub.status.idle":"2025-05-30T10:18:43.414586Z","shell.execute_reply.started":"2025-05-30T10:18:43.405412Z","shell.execute_reply":"2025-05-30T10:18:43.409248Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers\ntokenizer = transformers.PreTrainedTokenizerFast(tokenizer_object=tokenizer, special_tokens=special_tokens) ","metadata":{"id":"PGeqj391UvDZ","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:19:49.901523Z","iopub.execute_input":"2025-05-30T10:19:49.901832Z","iopub.status.idle":"2025-05-30T10:20:16.517239Z","shell.execute_reply.started":"2025-05-30T10:19:49.901806Z","shell.execute_reply":"2025-05-30T10:20:16.512327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.bos_token = \"<s>\"\ntokenizer.bos_token_id = tokenizer.convert_tokens_to_ids(\"<s>\")\ntokenizer.pad_token = \"<pad>\"\ntokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\"<pad>\")\ntokenizer.eos_token = \"</s>\"\ntokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(\"</s>\")\ntokenizer.unk_token = \"<unk>\"\ntokenizer.unk_token_id = tokenizer.convert_tokens_to_ids(\"<unk>\")\ntokenizer.cls_token = \"<cls>\"\ntokenizer.cls_token_id = tokenizer.convert_tokens_to_ids(\"<cls>\")\ntokenizer.sep_token = \"<sep>\"\ntokenizer.sep_token_id = tokenizer.convert_tokens_to_ids(\"<sep>\")\ntokenizer.mask_token = \"<mask>\"\ntokenizer.mask_token_id = tokenizer.convert_tokens_to_ids(\"<mask>\")","metadata":{"id":"VNl7xv6tWM6h","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:20:16.545866Z","iopub.execute_input":"2025-05-30T10:20:16.546181Z","iopub.status.idle":"2025-05-30T10:20:16.555836Z","shell.execute_reply.started":"2025-05-30T10:20:16.546158Z","shell.execute_reply":"2025-05-30T10:20:16.552320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!mkdir HeartBert\ntokenizer.save_pretrained(address)","metadata":{"id":"axnbFx0JXPOi","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:20:16.558995Z","iopub.execute_input":"2025-05-30T10:20:16.559227Z","iopub.status.idle":"2025-05-30T10:20:16.639097Z","shell.execute_reply.started":"2025-05-30T10:20:16.559204Z","shell.execute_reply":"2025-05-30T10:20:16.634082Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2- Heart MLM Model ","metadata":{"id":"pNDLrd0Kwsu7"}},{"cell_type":"markdown","source":"# HeartBert---training phase","metadata":{"id":"2YdPEFn9w50q"}},{"cell_type":"code","source":"# # Check that we have a GPU (for colab)\n# !nvidia-smi","metadata":{"id":"kD140sFjh0LQ","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:29:20.782811Z","iopub.execute_input":"2025-05-30T09:29:20.783034Z","iopub.status.idle":"2025-05-30T09:29:20.792043Z","shell.execute_reply.started":"2025-05-30T09:29:20.783013Z","shell.execute_reply":"2025-05-30T09:29:20.786687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check that PyTorch sees it\n# import torch\n# torch.cuda.is_available()","metadata":{"id":"VNZZs-r6iKAV","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:29:20.794106Z","iopub.execute_input":"2025-05-30T09:29:20.794322Z","iopub.status.idle":"2025-05-30T09:29:20.807263Z","shell.execute_reply.started":"2025-05-30T09:29:20.794302Z","shell.execute_reply":"2025-05-30T09:29:20.801578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import tensorflow as tf\n# print(\"TPU devices:\", tf.config.experimental.list_logical_devices('TPU'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:29:20.808766Z","iopub.execute_input":"2025-05-30T09:29:20.808961Z","iopub.status.idle":"2025-05-30T09:29:20.816014Z","shell.execute_reply.started":"2025-05-30T09:29:20.808942Z","shell.execute_reply":"2025-05-30T09:29:20.813091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import math, re, os\n# import tensorflow as tf\n# import numpy as np\n# from matplotlib import pyplot as plt\n# from kaggle_datasets import KaggleDatasets\n# from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n# print(\"Tensorflow version \" + tf.__version__)\n# AUTO = tf.data.experimental.AUTOTUNE\n\n# # Detect TPU, return appropriate distribution strategy\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n#     print('Running on TPU ', tpu.master())\n# except ValueError:\n#     tpu = None\n\n# if tpu:\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# else:\n#     strategy = tf.distribute.get_strategy() \n\n# print(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:29:20.818091Z","iopub.execute_input":"2025-05-30T09:29:20.818347Z","iopub.status.idle":"2025-05-30T09:29:20.829777Z","shell.execute_reply.started":"2025-05-30T09:29:20.818313Z","shell.execute_reply":"2025-05-30T09:29:20.825285Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2-1-  Model Config \n","metadata":{"id":"qxkrbo8yzUMr"}},{"cell_type":"code","source":"from transformers import RobertaConfig\n\nconfig = RobertaConfig(\n    vocab_size=52_000,\n    max_position_embeddings=514,\n    num_attention_heads=12,\n    num_hidden_layers=6,\n    type_vocab_size=1,\n)","metadata":{"id":"LTXXutqeDzPi","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:20:22.849189Z","iopub.execute_input":"2025-05-30T10:20:22.849503Z","iopub.status.idle":"2025-05-30T10:20:24.710350Z","shell.execute_reply.started":"2025-05-30T10:20:22.849476Z","shell.execute_reply":"2025-05-30T10:20:24.705154Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2-2- Tokenizer","metadata":{"id":"JP9pG6_w1aq-"}},{"cell_type":"code","source":"from transformers import TFAutoModel, AutoTokenizer\n\naddress = '/kaggle/working/tokenizer_pretrained'\ntokenizer = AutoTokenizer.from_pretrained(address) ","metadata":{"id":"4keFBUjQFOD1","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:20:28.039298Z","iopub.execute_input":"2025-05-30T10:20:28.039598Z","iopub.status.idle":"2025-05-30T10:20:48.193810Z","shell.execute_reply.started":"2025-05-30T10:20:28.039574Z","shell.execute_reply":"2025-05-30T10:20:48.188077Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2-3- MLM definition ","metadata":{"id":"mHXMRACL1wI-"}},{"cell_type":"code","source":"from transformers import RobertaForMaskedLM\n\nmodel = RobertaForMaskedLM(config=config)\nprint(model.num_parameters())","metadata":{"id":"BzMqR-dzF4Ro","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:21:59.888646Z","iopub.execute_input":"2025-05-30T10:21:59.888972Z","iopub.status.idle":"2025-05-30T10:22:01.470270Z","shell.execute_reply.started":"2025-05-30T10:21:59.888943Z","shell.execute_reply":"2025-05-30T10:22:01.464964Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2-4- Dataset Generation","metadata":{"id":"URwYg4gP2ZZ6"}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport os\nfrom typing import Dict\nimport torch\n\nclass LineByLineTextDataset(Dataset):\n    # tokenizer: PreTrainedTokenizer,\n    def __init__(self, file_path: str, block_size: int):\n        if os.path.isfile(file_path) is False:\n            raise ValueError(f\"Input file path {file_path} not found\")\n        \n        lines= []\n        with open(file_path, encoding=\"utf-8\") as f:\n            # lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n            for line in f:\n              line= line.strip('\\n')\n              if len(line)!=0:\n                if line.isspace():\n                  print('this line contains space!')\n                lines.append(line)\n\n        batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)\n        self.examples = batch_encoding[\"input_ids\"]\n        self.examples = [{\"input_ids\": torch.tensor(e, dtype=torch.long)} for e in self.examples]\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, i) -> Dict[str, torch.tensor]:\n        return self.examples[i]\n","metadata":{"id":"hgs_meAf19pA","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:22:05.050863Z","iopub.execute_input":"2025-05-30T10:22:05.051163Z","iopub.status.idle":"2025-05-30T10:22:05.064298Z","shell.execute_reply.started":"2025-05-30T10:22:05.051141Z","shell.execute_reply":"2025-05-30T10:22:05.059789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n########## Replace your file_path to train, val, and test files\nbs = 128\n\ntrain_dataset = LineByLineTextDataset(\n    # tokenizer=tokenizer,\n    file_path= '/kaggle/input/ecgbert/250 Hz/ecg_train.csv',\n    block_size=bs,\n)\n\neval_dataset = LineByLineTextDataset(\n    # tokenizer=tokenizer,\n    file_path='/kaggle/input/ecgbert/250 Hz/ecg_val.csv',\n    block_size=bs,\n)\n\ntest_dataset = LineByLineTextDataset(\n    # tokenizer=tokenizer,\n    file_path='/kaggle/input/ecgbert/250 Hz/ecg_test.csv',\n    block_size=bs,\n)","metadata":{"id":"GlvP_A-THEEl","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:22:09.231814Z","iopub.execute_input":"2025-05-30T10:22:09.232096Z","iopub.status.idle":"2025-05-30T10:22:12.916837Z","shell.execute_reply.started":"2025-05-30T10:22:09.232072Z","shell.execute_reply":"2025-05-30T10:22:12.912227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:22:14.050523Z","iopub.execute_input":"2025-05-30T10:22:14.050850Z","iopub.status.idle":"2025-05-30T10:22:14.065412Z","shell.execute_reply.started":"2025-05-30T10:22:14.050822Z","shell.execute_reply":"2025-05-30T10:22:14.059362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n)","metadata":{"id":"zTgWPa9Dipk2","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:22:16.278336Z","iopub.execute_input":"2025-05-30T10:22:16.278661Z","iopub.status.idle":"2025-05-30T10:22:16.344169Z","shell.execute_reply.started":"2025-05-30T10:22:16.278629Z","shell.execute_reply":"2025-05-30T10:22:16.338649Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2-5- Train","metadata":{"id":"E9qroc8a3TMe"}},{"cell_type":"code","source":"######################################################################################\n################################### ATTENTION!########################################\n### If this is the first time you run, run this cell; otherwise, skip this cell. ###\n######################################################################################\n\nimport pickle\n\npath_loss_eval = 'loss_eval.pickle' #path to loss_eval.pickle\npath_loss_train = 'loss_train.pickle' #path to loss_train.pickle\n\nloss_eval = []\nloss_train = []\n\nwith open(path_loss_train, 'wb') as handle:\n  pickle.dump(loss_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\nwith open(path_loss_eval, 'wb') as handle:\n  pickle.dump(loss_eval, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"id":"3F3Up_AETv0P","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:22:20.471546Z","iopub.execute_input":"2025-05-30T10:22:20.471857Z","iopub.status.idle":"2025-05-30T10:22:20.484308Z","shell.execute_reply.started":"2025-05-30T10:22:20.471831Z","shell.execute_reply":"2025-05-30T10:22:20.479992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainerCallback\nimport pickle\n\npath_loss_eval = 'loss_eval.pickle' #path to loss_eval.pickle\npath_loss_train = 'loss_train.pickle' #path to loss_train.pickle\n\n\nwith open(path_loss_train, 'rb') as handle:\n  loss_train = pickle.load(handle)\n\nwith open(path_loss_eval, 'rb') as handle:\n  loss_eval = pickle.load(handle)\n\n# the server can be lost in the middle of saving losses so we use the minimum length of stored losses between train and eval\nif len(loss_train)!=len(loss_eval):\n  print('difference between length of train loss and eval loss :(')\n  loss_train = loss_train[0:min(len(loss_train),len(loss_eval))]\n  loss_eval = loss_eval[0:min(len(loss_train),len(loss_eval))]\n\nclass PrinterCallback(TrainerCallback):\n  def on_log(self, args, state, control, logs=None, **kwargs):\n      print('my logs:',logs)\n      if 'loss' in logs.keys():\n        loss_train.append(logs['loss'])\n        with open(path_loss_train, 'wb') as handle:\n          pickle.dump(loss_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n      elif 'eval_loss' in logs.keys():\n        loss_eval.append(logs['eval_loss'])\n        with open(path_loss_eval, 'wb') as handle:\n          pickle.dump(loss_eval, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","metadata":{"id":"0sFPNLqIUq2m","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:22:20.923711Z","iopub.execute_input":"2025-05-30T10:22:20.924011Z","iopub.status.idle":"2025-05-30T10:22:20.989758Z","shell.execute_reply.started":"2025-05-30T10:22:20.923986Z","shell.execute_reply":"2025-05-30T10:22:20.985243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install --upgrade transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:22:13.372763Z","iopub.execute_input":"2025-05-30T08:22:13.373113Z","iopub.status.idle":"2025-05-30T08:22:13.382807Z","shell.execute_reply.started":"2025-05-30T08:22:13.373087Z","shell.execute_reply":"2025-05-30T08:22:13.378114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install transformers==4.38.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:22:13.769813Z","iopub.execute_input":"2025-05-30T08:22:13.770154Z","iopub.status.idle":"2025-05-30T08:22:13.780179Z","shell.execute_reply.started":"2025-05-30T08:22:13.770128Z","shell.execute_reply":"2025-05-30T08:22:13.775615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install accelerate -U","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:22:14.241573Z","iopub.execute_input":"2025-05-30T08:22:14.241916Z","iopub.status.idle":"2025-05-30T08:22:14.252433Z","shell.execute_reply.started":"2025-05-30T08:22:14.241889Z","shell.execute_reply":"2025-05-30T08:22:14.246797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from transformers import Trainer, TrainerCallback\n# import torch_xla.core.xla_model as xm\n\n# class PrinterCallback(TrainerCallback):\n#     def on_epoch_end(self, args, state, control, **kwargs):\n#         print(f\"Epoch {state.epoch} done.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\nfrom transformers import EarlyStoppingCallback, IntervalStrategy\n\naddress = 'training_arguments'\n\ntraining_args = TrainingArguments(\n    output_dir= address, #your address to the output directory\n    report_to = 'none',\n    overwrite_output_dir=True,\n    num_train_epochs=400,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    # gradient_accumulation_steps = 1,\n    save_steps=10_000,\n    save_total_limit=2,\n    prediction_loss_only=True,\n    #logging_steps=50,\n    logging_strategy = 'epoch',                            \n    eval_strategy = IntervalStrategy.EPOCH, \n    save_strategy = 'epoch',\n    metric_for_best_model = 'loss',\n    load_best_model_at_end=True,\n)","metadata":{"id":"YpvnFFmZJD-N","trusted":true,"execution":{"execution_failed":"2025-05-30T10:27:27.314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install \"transformers[torch]\" --upgrade\n# !pip install accelerate --upgrade\n# !pip install datasets --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:22:23.238514Z","iopub.execute_input":"2025-05-30T08:22:23.238891Z","iopub.status.idle":"2025-05-30T08:22:23.251664Z","shell.execute_reply.started":"2025-05-30T08:22:23.238861Z","shell.execute_reply":"2025-05-30T08:22:23.244597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(\n    model=model.to('cuda'),\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    callbacks = [PrinterCallback, EarlyStoppingCallback(early_stopping_patience=50, early_stopping_threshold=0)]\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-30T10:27:27.314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######################################################################################\n################################### ATTENTION!########################################\n### If this is the first time you run, run this cell (skip the next); otherwise, skip this cell (run the next). ###\n######################################################################################\n\nimport time\n\nstart_time = time.time()\n\ntrainer.train()  # only first time running \n\nend_time = time.time()\nprint(f\"Training took {end_time - start_time:.2f} seconds\")","metadata":{"id":"VmaHZXzmkNtJ","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:00:30.412578Z","iopub.execute_input":"2025-04-28T17:00:30.413130Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ntrainer.train(resume_from_checkpoint=True) ","metadata":{"id":"sOCYcbQPX-8D","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2-6- Evaluation","metadata":{"id":"J6gvCWdDiz-S"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np","metadata":{"id":"0eEF7m8YpuTo","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T20:24:47.464951Z","iopub.execute_input":"2025-04-27T20:24:47.465218Z","iopub.status.idle":"2025-04-27T20:24:47.468719Z","shell.execute_reply.started":"2025-04-27T20:24:47.465202Z","shell.execute_reply":"2025-04-27T20:24:47.468021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.rcParams.update({\n    'font.family': 'Times New Roman',\n    'font.size': 16,\n    'axes.titlesize': 16,\n    'axes.labelsize': 16,\n    'xtick.labelsize': 16,\n    'ytick.labelsize': 16,\n})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(np.arange(1,len(loss_train)+1),loss_train,)\nplt.plot(np.arange(1,len(loss_eval)+1),loss_eval,)\nplt.legend(['loss_train','loss_eval'])\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.title('loss per epoch')\n# plt.show()\nplt.savefig('loss_per_epoch.png')","metadata":{"id":"RPprr3GCIveY","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T20:24:48.138906Z","iopub.execute_input":"2025-04-27T20:24:48.139172Z","iopub.status.idle":"2025-04-27T20:24:48.445750Z","shell.execute_reply.started":"2025-04-27T20:24:48.139154Z","shell.execute_reply":"2025-04-27T20:24:48.444965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EvaluationMetrics:\n  def __init__(self,):\n    pass\n    #crossentropy is the loss of model wrt input\n  def perplexity(self, crossentropy):\n    return np.exp(crossentropy)","metadata":{"id":"33T6qtDsIzOe","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T20:24:56.071797Z","iopub.execute_input":"2025-04-27T20:24:56.072478Z","iopub.status.idle":"2025-04-27T20:24:56.076715Z","shell.execute_reply.started":"2025-04-27T20:24:56.072453Z","shell.execute_reply":"2025-04-27T20:24:56.075782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# an instance of class EvaluationMetrics\nevaluation_metrics = EvaluationMetrics()","metadata":{"id":"97BvUKCVI2i_","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T20:24:57.944135Z","iopub.execute_input":"2025-04-27T20:24:57.944430Z","iopub.status.idle":"2025-04-27T20:24:57.948885Z","shell.execute_reply.started":"2025-04-27T20:24:57.944391Z","shell.execute_reply":"2025-04-27T20:24:57.948178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(np.arange(1,len(loss_train)+1),evaluation_metrics.perplexity(loss_train))\nplt.plot(np.arange(1,len(loss_eval)+1),evaluation_metrics.perplexity(loss_eval))\nplt.legend(['ppl_train','ppl_eval'])\nplt.xlabel('epoch')\nplt.ylabel('ppl')\nplt.title('ppl per epoch')\nplt.show()","metadata":{"id":"dwZ6LIwuI5rr","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T20:25:01.548100Z","iopub.execute_input":"2025-04-27T20:25:01.548459Z","iopub.status.idle":"2025-04-27T20:25:01.718779Z","shell.execute_reply.started":"2025-04-27T20:25:01.548437Z","shell.execute_reply":"2025-04-27T20:25:01.718105Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"test","metadata":{"id":"IKq7yk1qJKhO"}},{"cell_type":"code","source":"# copy loss_eval because when we call trainer.evaluate() on_log append loss of test to list of loss_eval so we have loss of eval loss on first part of the list and loss of test at the end part of the list.\n# to not confront to this problem first we clear the content of loss_eval and after we use it for test set, we bring back the original value of loss_eval.\nimport copy\nloss_eval_temp = copy.deepcopy(loss_eval)\nloss_eval = []\n\nloss_test = []\nfor i in range(len(test_dataset)): #check\n  # print(sample)\n  o = trainer.evaluate(eval_dataset=test_dataset[i:i+1],)\n  print(o['eval_loss'])\n  loss_test.append(o['eval_loss'])\n\nloss_eval = copy.deepcopy(loss_eval_temp)\n\nplt.plot(np.arange(1,len(loss_test)+1),loss_test)\nplt.legend(['loss_test'])\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.title('loss per epoch')\n# plt.show()\nplt.savefig('test_loss_per_epoch.png')\n","metadata":{"id":"0B2fsM-XJKGr","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T20:25:33.953656Z","iopub.execute_input":"2025-04-27T20:25:33.954567Z","iopub.status.idle":"2025-04-27T20:25:35.531269Z","shell.execute_reply.started":"2025-04-27T20:25:33.954541Z","shell.execute_reply":"2025-04-27T20:25:35.530459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(np.arange(1,len(loss_test)+1),evaluation_metrics.perplexity(loss_test))\nplt.legend(['ppl_test'])\nplt.xlabel('sample')\nplt.ylabel('ppl')\nplt.title('ppl per sample')\n# plt.show()\nplt.savefig('ppl_per_sample.png')","metadata":{"id":"xTEVI4OgJPhX","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T20:26:50.030695Z","iopub.execute_input":"2025-04-27T20:26:50.031209Z","iopub.status.idle":"2025-04-27T20:26:50.276914Z","shell.execute_reply.started":"2025-04-27T20:26:50.031186Z","shell.execute_reply":"2025-04-27T20:26:50.276146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss_eval_temp = copy.deepcopy(loss_eval)\nloss_eval = []\n# calculating perplexity of model using  test set. in eval_dataset you can use train, dev or test set\nprint('PPL:',evaluation_metrics.perplexity(trainer.evaluate(eval_dataset=test_dataset,)['eval_loss']))\n\nloss_eval = copy.deepcopy(loss_eval_temp)","metadata":{"id":"Qu7AObFYJSyH","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T20:27:03.830872Z","iopub.execute_input":"2025-04-27T20:27:03.831157Z","iopub.status.idle":"2025-04-27T20:27:04.148440Z","shell.execute_reply.started":"2025-04-27T20:27:03.831136Z","shell.execute_reply":"2025-04-27T20:27:04.147846Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2-7 Saving model","metadata":{"id":"h2FsdMVJ4Qjb"}},{"cell_type":"code","source":"address = 'model'\ntrainer.save_model(address)\n\n# Saving model checkpoint to ./HeartBert/mlm_model\n# Configuration saved in ./HeartBert/mlm_model/config.json\n# Model weights saved in ./HeartBert/mlm_model/pytorch_model.bin","metadata":{"id":"QDNgPls7_l13","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3- Test\n\n","metadata":{"id":"g5mhwS9L5QQT"}},{"cell_type":"markdown","source":"Aside from looking at the training and eval losses going down, the easiest way to check whether our language model is learning anything interesting is via the `FillMaskPipeline`.\n\nPipelines are simple wrappers around tokenizers and models, and the 'fill-mask' one will let you input a sequence containing a masked token (here, `<mask>`) and return a list of the most probable filled sequences, with their probabilities.\n\n","metadata":{"id":"iIQJ8ND_AEhl"}},{"cell_type":"code","source":"# from transformers import pipeline\n\n# fill_mask = pipeline(\n#     \"fill-mask\",\n#     model= address,\n#     tokenizer= address\n# )","metadata":{"id":"ltXgXyCbAJLY"},"outputs":[],"execution_count":null}]}