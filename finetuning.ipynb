{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11597704,"sourceType":"datasetVersion","datasetId":7273143},{"sourceId":11956150,"sourceType":"datasetVersion","datasetId":7273465},{"sourceId":12036167,"sourceType":"datasetVersion","datasetId":7267352},{"sourceId":412692,"sourceType":"modelInstanceVersion","modelInstanceId":302573,"modelId":323072},{"sourceId":417009,"sourceType":"modelInstanceVersion","modelInstanceId":340211,"modelId":323072},{"sourceId":423410,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":345058,"modelId":323072},{"sourceId":430597,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":350979,"modelId":323072},{"sourceId":430610,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":350989,"modelId":323072}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install neurokit2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:39.573097Z","iopub.execute_input":"2025-06-08T15:44:39.573318Z","iopub.status.idle":"2025-06-08T15:44:44.350991Z","shell.execute_reply.started":"2025-06-08T15:44:39.573301Z","shell.execute_reply":"2025-06-08T15:44:44.350102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport ast\nimport matplotlib.pyplot as plt\nimport neurokit2 as nk\nimport json\nimport os","metadata":{"id":"bec6975b","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:44.352243Z","iopub.execute_input":"2025-06-08T15:44:44.352483Z","iopub.status.idle":"2025-06-08T15:44:48.163810Z","shell.execute_reply.started":"2025-06-08T15:44:44.352461Z","shell.execute_reply":"2025-06-08T15:44:48.163280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import mne\n# import numpy as np\n# import pandas as pd\n# from scipy.signal import find_peaks\n\n# mne.set_log_level('WARNING')\n\n# all_data = []\n\n# def extract_periods(signal, distance_sec, sfreq, num_periods=5):\n#     # Find peaks\n#     peaks, _ = find_peaks(signal, distance=int(distance_sec * sfreq * 0.8))\n#     if len(peaks) < num_periods + 1:\n#         raise ValueError(\"Not enough peaks to extract the required number of periods.\")\n    \n#     periods = []\n#     for i in range(num_periods):\n#         start = peaks[i]\n#         end = peaks[i+1]\n#         periods.append(signal[start:end])\n#     return periods\n\n# def process_files(folder_path, label):\n#     file_names = os.listdir(folder_path)\n#     for file_name in file_names:\n#         if file_name.endswith('.edf'):\n#             try:\n#                 file_path = os.path.join(folder_path, file_name)\n#                 data = mne.io.read_raw_edf(file_path, preload=True)\n                \n#                 raw_data = data.get_data()\n#                 channels = data.ch_names\n#                 sfreq = data.info['sfreq']\n                \n#                 ecg_channels = ['ECG I', 'ECG II', 'ECG V1', 'ECG V2', 'ECG V3', 'ECG V4', 'ECG V5', 'ECG V6']\n                \n#                 missing_channels = [ch for ch in ecg_channels if ch not in channels]\n#                 if missing_channels:\n#                     print(f\"File {file_path} is missing channels: {missing_channels}\")\n#                     continue\n                \n#                 ecg_processed = []\n#                 ecg_pieces = []\n#                 for ch in ecg_channels:\n#                     channel_data = raw_data[channels.index(ch)]\n#                     channel_data = (channel_data - np.mean(channel_data)) / np.std(channel_data)\n                    \n#                     try:\n#                         periods = extract_periods(channel_data, distance_sec=0.6, sfreq=sfreq, num_periods=5)\n                        \n#                         for period in periods:\n#                             ecg_pieces.append(period)\n#                     except Exception as e:\n#                         print(f\"Could not extract periods from {file_name} channel {ch}: {e}\")\n\n#                 if ecg_pieces:\n#                     ecg_signal = np.hstack(ecg_pieces)\n#                     all_data.append({'signal': ecg_signal.tolist(), 'label': label})\n                \n#             except Exception as e:\n#                 print(f'Error with file {file_name}: {e}')\n#                 continue\n\n# # Process Amy files (label 1)\n# process_files(folder_path='/kaggle/input/finetuning-dataset/LFV', label=1)\n\n# # Process noAMY files (label 0)\n# process_files(folder_path='/kaggle/input/finetuning-dataset/noLFV', label=0)\n\n# # Create final DataFrame\n# df = pd.DataFrame(all_data)\n\n# # Save to ecg_train.csv\n# df.to_pickle('ecg_tune.pkl')\n\n# print(\"Successfully whatever!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:48.164692Z","iopub.execute_input":"2025-06-08T15:44:48.165362Z","iopub.status.idle":"2025-06-08T15:44:48.169714Z","shell.execute_reply.started":"2025-06-08T15:44:48.165335Z","shell.execute_reply":"2025-06-08T15:44:48.169018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# maxm = 0\n# for i in range (104000):\n#     if np.int32(df.iloc[i, 0].shape[0]) > maxm:\n#         maxm = df.iloc[i, 0].shape\n# print(maxm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:48.171884Z","iopub.execute_input":"2025-06-08T15:44:48.172326Z","iopub.status.idle":"2025-06-08T15:44:48.190633Z","shell.execute_reply.started":"2025-06-08T15:44:48.172307Z","shell.execute_reply":"2025-06-08T15:44:48.190082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df = pd.read_pickle('/kaggle/input/finetuning-dataset/heartbeats.pkl')\n# # df.shape\n# n = np.linspace(0, 100, len(df.iloc[150][0]))\n# plt.plot(n, df.iloc[150][0])","metadata":{"id":"lClpN762IhKm","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:48.191339Z","iopub.execute_input":"2025-06-08T15:44:48.191524Z","iopub.status.idle":"2025-06-08T15:44:48.204077Z","shell.execute_reply.started":"2025-06-08T15:44:48.191508Z","shell.execute_reply":"2025-06-08T15:44:48.203536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_pickle('/kaggle/input/finetuning-dataset/ecg_tune.pkl')\ndf.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:48.204819Z","iopub.execute_input":"2025-06-08T15:44:48.205040Z","iopub.status.idle":"2025-06-08T15:44:49.121010Z","shell.execute_reply.started":"2025-06-08T15:44:48.205016Z","shell.execute_reply":"2025-06-08T15:44:49.120306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:49.121797Z","iopub.execute_input":"2025-06-08T15:44:49.122122Z","iopub.status.idle":"2025-06-08T15:44:49.186416Z","shell.execute_reply.started":"2025-06-08T15:44:49.122099Z","shell.execute_reply":"2025-06-08T15:44:49.185713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_db_new = df[df['label']==0].head(0)\nfor i in range(2):\n total_db_new = pd.concat([total_db_new,df[df['label']==i].sample(n=908)],ignore_index=True)\n# total_db_new = df","metadata":{"id":"wnuAlmWnF4e0","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:49.187233Z","iopub.execute_input":"2025-06-08T15:44:49.187418Z","iopub.status.idle":"2025-06-08T15:44:49.199558Z","shell.execute_reply.started":"2025-06-08T15:44:49.187403Z","shell.execute_reply":"2025-06-08T15:44:49.198938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def resample_signal(row):\n    original_signal = row['signal']\n    resampled_signal = nk.signal_resample(original_signal, sampling_rate=500, desired_sampling_rate= 250, method=\"FFT\")\n    return pd.Series({'signal': resampled_signal, 'label': row['label']})","metadata":{"id":"0d576bd3","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:49.200340Z","iopub.execute_input":"2025-06-08T15:44:49.200578Z","iopub.status.idle":"2025-06-08T15:44:49.204657Z","shell.execute_reply.started":"2025-06-08T15:44:49.200555Z","shell.execute_reply":"2025-06-08T15:44:49.203940Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resampled_df = total_db_new.apply(resample_signal, axis=1)\ndel total_db_new\ntotal_db_new = resampled_df","metadata":{"id":"iMAWHE6QLPPl","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:49.205367Z","iopub.execute_input":"2025-06-08T15:44:49.205579Z","iopub.status.idle":"2025-06-08T15:44:50.494323Z","shell.execute_reply.started":"2025-06-08T15:44:49.205564Z","shell.execute_reply":"2025-06-08T15:44:50.493510Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Split and Normalization","metadata":{"id":"g9fqfIyXE6CB"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, temp_df = train_test_split(total_db_new, test_size=0.3, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=(2/3), random_state=42)\n\ntrain_df.reset_index(drop=True, inplace=True)\nval_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)","metadata":{"id":"ymYewx2pE8E7","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:50.495187Z","iopub.execute_input":"2025-06-08T15:44:50.495433Z","iopub.status.idle":"2025-06-08T15:44:50.502861Z","shell.execute_reply.started":"2025-06-08T15:44:50.495412Z","shell.execute_reply":"2025-06-08T15:44:50.502369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_max_min(current_df):\n  min_value = current_df['signal'].apply(lambda x: min(x)).min()\n  max_value = current_df['signal'].apply(lambda x: max(x)).max()\n  return min_value, max_value\n\nmin_train_df, max_train_df = find_max_min(train_df)\nprint(min_train_df, max_train_df)\n\nmin_val_df, max_val_df = find_max_min(val_df)\nprint(min_val_df, max_val_df)\n\nmin_test_df, max_test_df = find_max_min(test_df)\nprint(min_test_df, max_test_df)","metadata":{"id":"6Q_TwZaddElo","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:50.503489Z","iopub.execute_input":"2025-06-08T15:44:50.503775Z","iopub.status.idle":"2025-06-08T15:44:50.948732Z","shell.execute_reply.started":"2025-06-08T15:44:50.503754Z","shell.execute_reply":"2025-06-08T15:44:50.948094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['signal'] = train_df['signal'].apply(lambda x: [(item - min_train_df) / (max_train_df - min_train_df) for item in x])\n\nval_df['signal'] = val_df['signal'].apply(lambda x: [(item - min_val_df) / (max_val_df - min_val_df) for item in x])\n\ntest_df['signal'] = test_df['signal'].apply(lambda x: [(item - min_test_df) / (max_test_df - min_test_df) for item in x])\n","metadata":{"id":"xQUEFk7yfB9m","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:50.949450Z","iopub.execute_input":"2025-06-08T15:44:50.949649Z","iopub.status.idle":"2025-06-08T15:44:51.989366Z","shell.execute_reply.started":"2025-06-08T15:44:50.949629Z","shell.execute_reply":"2025-06-08T15:44:51.988815Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Quantization\n","metadata":{"id":"rP54exdx7ZFF"}},{"cell_type":"code","source":"# from https://github.com/joaomrcarvalho/diffquantizer.git\n\nclass DiffQuantizer:\n    def __init__(self, alphabet_size, average_over=1, filter=False, breakpoints=None, use_diffs=True):\n\n        self.alphabet_size = alphabet_size\n        self.average_over = average_over\n        self.use_filter = filter\n        self.breakpoints = breakpoints\n        self.use_diffs = use_diffs\n        # print('self.breakpoints:',self.breakpoints)\n\n    def preprocess(self, tmp):\n        if self.average_over != 1:\n            tmp = self._average_over_n(tmp, self.average_over)\n\n        if self.use_filter:\n            tmp = self._filter_signal(tmp)\n\n        if self.use_diffs:\n            tmp = self._diff_signal(tmp)\n\n        return tmp\n\n    def perform_quantization(self, tmp, breakpoints=None):\n        self.breakpoints = breakpoints\n        # print('perform_quantization, self.breakpoints:',self.breakpoints)\n        tmp = self.preprocess(tmp)\n        result = self._quantize_with_breakpoints(tmp)\n        return result\n\n    def learn_breakpoints(self, arr):\n        res = self.preprocess(arr)\n\n        sorted_array = np.sort(res)\n\n        length = len(sorted_array)\n\n        probs = [1 / self.alphabet_size for _ in range(self.alphabet_size)]\n        cum_sum_breakpoints = [int(sum(probs[0:i + 1]) * length - 1) for i in range(len(probs))]\n        cum_sum_breakpoint_values = sorted_array[cum_sum_breakpoints]\n\n        cum_sum_breakpoint_values[-1] = 1e+100\n\n        self.breakpoints = cum_sum_breakpoint_values\n        # print('learn_breakpoints,self.breakpoints',self.breakpoints)\n\n        return cum_sum_breakpoint_values\n\n    # vectorized use\n    @staticmethod\n    def _breakpoint_to_letter(float_num, breakpoints):\n        # print('float_num:',float_num,'breakpoints:',breakpoints)\n        # print(list((breakpoints.index(obj) for obj in breakpoints if float_num < obj)))\n        int_val = next((breakpoints.index(obj) for obj in breakpoints if float_num < obj))\n        # print(int_val,list((breakpoints.index(obj) for obj in breakpoints if float_num < obj)))\n        # A + int_val\n        return chr(65 + int_val)\n\n    def _quantize_with_breakpoints(self, tmp):\n        breakpoints = self.breakpoints\n        vect_breakpoint_to_letter = np.vectorize(self._breakpoint_to_letter, excluded=['breakpoints'])\n        # print(tmp,breakpoints,self._breakpoint_to_letter,vect_breakpoint_to_letter)\n        tmp = vect_breakpoint_to_letter(tmp, breakpoints=list(breakpoints))\n        # tmp = vect_breakpoint_to_letter(tmp, breakpoints=breakpoints)\n        return tmp\n\n    @staticmethod\n    def _read_csv_file(input_file):\n        tmp_file_content = pd.read_csv(input_file, sep=\"\\n\", header=None, dtype=np.float64)\n        return np.array(tmp_file_content)\n\n    ## @staticmethod\n    # def _filter_signal(tmp):\n    #     return butter_lowpass_filter(tmp)\n\n    @staticmethod\n    def _average_over_n(tmp, n):\n        return np.array([np.average(tmp[i:i + n]) for i in range(0, len(tmp), n)])\n\n    @staticmethod\n    def _diff_signal(tmp):\n        res = np.diff(tmp)\n        return np.insert(res, 0, 0.0)","metadata":{"id":"T9cwSwdnUrl7","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:51.991800Z","iopub.execute_input":"2025-06-08T15:44:51.992449Z","iopub.status.idle":"2025-06-08T15:44:52.002671Z","shell.execute_reply.started":"2025-06-08T15:44:51.992431Z","shell.execute_reply":"2025-06-08T15:44:52.001928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\nimport csv\n\nclass Preprocessing():\n\n  def __init__(self, input_file, discretizition_factor, max_window_size):\n    self.input_file = input_file\n    self.discretizition_factor = discretizition_factor\n    self.max_window_size = max_window_size\n\n\n  def create_window(self):\n\n    ecg_window = [] #ecg_window: list of windows\n\n    mlen=0\n    ecg_list = self.input_file['signal']\n    if len(ecg_list) > mlen:\n      mlen= len(ecg_list)\n\n    if mlen > self.max_window_size:\n      window_size = self.max_window_size\n    else:\n      window_size = mlen\n\n    num_lines = math.floor(len(ecg_list)/window_size) #in this case: 1\n\n    for i in range(num_lines):\n      tmp_list = ecg_list[i*window_size:(i+1)*window_size]\n      ecg_window.append(tmp_list)\n\n    return ecg_window\n\n  def change_to_alphabet(self, quantizer, normalized_list):\n    qtz_signal = []\n    labels = []\n    qtz = quantizer\n    for i in range(len(normalized_list)): #i is for each line\n      r = qtz.perform_quantization(np.array(normalized_list[i]),breakpoints=qtz.breakpoints)\n      # print('result:',r.shape)\n      # make r from list of chars  to string a chars by ''.joint(r)\n      qtz_signal.append(''.join(r))\n      labels.append(self.input_file['label'])\n\n    return qtz_signal, labels","metadata":{"id":"FP5-zJfHbkdg","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:52.005406Z","iopub.execute_input":"2025-06-08T15:44:52.006079Z","iopub.status.idle":"2025-06-08T15:44:52.023176Z","shell.execute_reply.started":"2025-06-08T15:44:52.006060Z","shell.execute_reply":"2025-06-08T15:44:52.022478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### lloyd_max\ndef discretization_lloyd_max(discretizition_factor, total_data):\n  qtz_signal = []\n  labels = []\n  qtz=DiffQuantizer(alphabet_size=discretizition_factor,breakpoints=None,use_diffs=False)\n  qtz.learn_breakpoints(np.array(total_data))\n  return qtz","metadata":{"id":"OQTmc7tdvUeI","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:52.461610Z","iopub.execute_input":"2025-06-08T15:44:52.462351Z","iopub.status.idle":"2025-06-08T15:44:52.466076Z","shell.execute_reply.started":"2025-06-08T15:44:52.462327Z","shell.execute_reply":"2025-06-08T15:44:52.465337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_Preprocessing(db, max_window_size=4000, discretizition_factor=100):\n    discretizition_factor = discretizition_factor\n    max_window_size = max_window_size\n    nl_list, total_data = [], []\n    r_list, labels = [], []\n\n    for n in range(len(db)):\n      pre = Preprocessing(db.loc[n], discretizition_factor, max_window_size)\n      nl = pre.create_window()\n      nl_list.append(nl)\n\n    for i in range(len(nl_list)):\n        total_data.extend(nl_list[i][0])\n\n    quantize_max_lloyd_on_total_data = discretization_lloyd_max(discretizition_factor, total_data)\n\n    for n in range(len(db)):\n      pre = Preprocessing(db.loc[n], discretizition_factor, max_window_size)\n      nl = pre.create_window()\n      r, l = pre.change_to_alphabet(quantize_max_lloyd_on_total_data, nl)\n      r_list.extend(r)\n      labels.extend(l)\n    print(n, \" : done\")\n\n    return r_list, labels","metadata":{"id":"Q_D6QlzN-lNN","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:52.907885Z","iopub.execute_input":"2025-06-08T15:44:52.908181Z","iopub.status.idle":"2025-06-08T15:44:52.914234Z","shell.execute_reply.started":"2025-06-08T15:44:52.908134Z","shell.execute_reply":"2025-06-08T15:44:52.913429Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"`check max_window_size`","metadata":{"id":"Qaleio2RaHuG"}},{"cell_type":"code","source":"r_list_train, labels_train = run_Preprocessing(db=train_df, max_window_size=4000, discretizition_factor=100)\n\nr_list_val, labels_val = run_Preprocessing(db=val_df, max_window_size=4000, discretizition_factor=100)\n\nr_list_test, labels_test = run_Preprocessing(db=test_df, max_window_size=4000, discretizition_factor=100)","metadata":{"id":"8kfdJIhbOgzE","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:44:55.335709Z","iopub.execute_input":"2025-06-08T15:44:55.335981Z","iopub.status.idle":"2025-06-08T15:45:13.245745Z","shell.execute_reply.started":"2025-06-08T15:44:55.335962Z","shell.execute_reply":"2025-06-08T15:45:13.244965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(r_list_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:13.246979Z","iopub.execute_input":"2025-06-08T15:45:13.247218Z","iopub.status.idle":"2025-06-08T15:45:13.251948Z","shell.execute_reply.started":"2025-06-08T15:45:13.247202Z","shell.execute_reply":"2025-06-08T15:45:13.251285Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenizer","metadata":{"id":"P_EiiYPF0Yec"}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel","metadata":{"id":"fT5QosR96oJe","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:13.252733Z","iopub.execute_input":"2025-06-08T15:45:13.252961Z","iopub.status.idle":"2025-06-08T15:45:20.534884Z","shell.execute_reply.started":"2025-06-08T15:45:13.252941Z","shell.execute_reply":"2025-06-08T15:45:20.534102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer_ft = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"/kaggle/input/pretrained_heartbert/transformers/pt20k_250/1/model\")","metadata":{"id":"4nKlFTVqtMNr","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:20.536613Z","iopub.execute_input":"2025-06-08T15:45:20.537386Z","iopub.status.idle":"2025-06-08T15:45:20.741055Z","shell.execute_reply.started":"2025-06-08T15:45:20.537367Z","shell.execute_reply":"2025-06-08T15:45:20.740538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_function(examples, max_length = 512):\n    return tokenizer_ft(examples, padding=\"max_length\", truncation=True, max_length=max_length)","metadata":{"id":"Waf_u_mS6m2n","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:20.741712Z","iopub.execute_input":"2025-06-08T15:45:20.741938Z","iopub.status.idle":"2025-06-08T15:45:20.745876Z","shell.execute_reply.started":"2025-06-08T15:45:20.741921Z","shell.execute_reply":"2025-06-08T15:45:20.745224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_dataset_train = list(map(tokenize_function, r_list_train))\n\ntokenized_dataset_val = list(map(tokenize_function, r_list_val))\n\ntokenized_dataset_test = list(map(tokenize_function, r_list_test))","metadata":{"id":"wVCCbiBY6m2n","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:20.746542Z","iopub.execute_input":"2025-06-08T15:45:20.746716Z","iopub.status.idle":"2025-06-08T15:45:23.035063Z","shell.execute_reply.started":"2025-06-08T15:45:20.746701Z","shell.execute_reply":"2025-06-08T15:45:23.034188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(tokenized_dataset_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:25.340058Z","iopub.execute_input":"2025-06-08T15:45:25.340373Z","iopub.status.idle":"2025-06-08T15:45:25.345365Z","shell.execute_reply.started":"2025-06-08T15:45:25.340351Z","shell.execute_reply":"2025-06-08T15:45:25.344658Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## input_ids, attention_masks, labels","metadata":{"id":"niQm13G2qEfe"}},{"cell_type":"code","source":"def return_ids(tokenized_dataset):\n\n  dataset = []\n  input_ids = []\n  attention_masks = []\n\n  for i in range(len(tokenized_dataset)):\n\n      dataset.append(tokenized_dataset[i])\n      del dataset[-1]['token_type_ids']\n\n      input_ids.append(np.array(dataset[-1]['input_ids'], dtype=np.int32))\n\n      attention_masks.append(np.array(dataset[-1]['attention_mask'], dtype=bool))\n\n      dataset = []\n\n  input_ids = np.array(input_ids)\n  attention_masks = np.array(attention_masks)\n\n  return input_ids, attention_masks","metadata":{"id":"lGGbYR7spHjt","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:26.547787Z","iopub.execute_input":"2025-06-08T15:45:26.548334Z","iopub.status.idle":"2025-06-08T15:45:26.552953Z","shell.execute_reply.started":"2025-06-08T15:45:26.548310Z","shell.execute_reply":"2025-06-08T15:45:26.552314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_ids_train, attention_masks_train = return_ids(tokenized_dataset_train)\n\ninput_ids_val, attention_masks_val = return_ids(tokenized_dataset_val)\n\ninput_ids_test, attention_masks_test = return_ids(tokenized_dataset_test)","metadata":{"id":"NZFMyJCntiwk","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:29.424577Z","iopub.execute_input":"2025-06-08T15:45:29.424849Z","iopub.status.idle":"2025-06-08T15:45:29.535695Z","shell.execute_reply.started":"2025-06-08T15:45:29.424831Z","shell.execute_reply":"2025-06-08T15:45:29.534959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels_train = np.array(labels_train,dtype=np.int8)\n\nlabels_val = np.array(labels_val,dtype=np.int8)\n\nlabels_test = np.array(labels_test,dtype=np.int8)","metadata":{"id":"OG17Rq5tuq0K","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:29.821466Z","iopub.execute_input":"2025-06-08T15:45:29.821751Z","iopub.status.idle":"2025-06-08T15:45:29.826061Z","shell.execute_reply.started":"2025-06-08T15:45:29.821731Z","shell.execute_reply":"2025-06-08T15:45:29.825433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(labels_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:31.912451Z","iopub.execute_input":"2025-06-08T15:45:31.912943Z","iopub.status.idle":"2025-06-08T15:45:31.917549Z","shell.execute_reply.started":"2025-06-08T15:45:31.912922Z","shell.execute_reply":"2025-06-08T15:45:31.916947Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DataLoader","metadata":{"id":"bEppEuYow1Wy"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader","metadata":{"id":"ajDUtGJYPNws","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:32.414721Z","iopub.execute_input":"2025-06-08T15:45:32.415295Z","iopub.status.idle":"2025-06-08T15:45:32.419021Z","shell.execute_reply.started":"2025-06-08T15:45:32.415272Z","shell.execute_reply":"2025-06-08T15:45:32.418336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"id":"eQq23H8MO_6q","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:32.749487Z","iopub.execute_input":"2025-06-08T15:45:32.749760Z","iopub.status.idle":"2025-06-08T15:45:32.804971Z","shell.execute_reply.started":"2025-06-08T15:45:32.749739Z","shell.execute_reply":"2025-06-08T15:45:32.804197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bert_model = AutoModel.from_pretrained(\"/kaggle/input/pretrained_heartbert/transformers/pt20k_250/1/model\")","metadata":{"id":"j_ovU5k_FQvH","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:32.880667Z","iopub.execute_input":"2025-06-08T15:45:32.881396Z","iopub.status.idle":"2025-06-08T15:45:55.033870Z","shell.execute_reply.started":"2025-06-08T15:45:32.881371Z","shell.execute_reply":"2025-06-08T15:45:55.033073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bert_model = bert_model.to(device)","metadata":{"id":"SvpgO2WaO06I","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:55.035338Z","iopub.execute_input":"2025-06-08T15:45:55.035865Z","iopub.status.idle":"2025-06-08T15:45:58.891801Z","shell.execute_reply.started":"2025-06-08T15:45:55.035846Z","shell.execute_reply":"2025-06-08T15:45:58.890980Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bert_model.eval()","metadata":{"id":"n133rpUCyZ-B","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:58.892589Z","iopub.execute_input":"2025-06-08T15:45:58.893341Z","iopub.status.idle":"2025-06-08T15:45:58.898783Z","shell.execute_reply.started":"2025-06-08T15:45:58.893312Z","shell.execute_reply":"2025-06-08T15:45:58.898213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for param in bert_model.parameters():\n  param.requires_grad = False","metadata":{"id":"Kr2lzUUCL3Is","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:58.900547Z","iopub.execute_input":"2025-06-08T15:45:58.900954Z","iopub.status.idle":"2025-06-08T15:45:58.916158Z","shell.execute_reply.started":"2025-06-08T15:45:58.900927Z","shell.execute_reply":"2025-06-08T15:45:58.915584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 8","metadata":{"id":"-8-Cp5p8xUbE","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:58.916756Z","iopub.execute_input":"2025-06-08T15:45:58.916925Z","iopub.status.idle":"2025-06-08T15:45:58.930058Z","shell.execute_reply.started":"2025-06-08T15:45:58.916910Z","shell.execute_reply":"2025-06-08T15:45:58.929535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_ = TensorDataset(torch.tensor(input_ids_train), torch.tensor(labels_train),\n                         torch.tensor(attention_masks_train))\ndataloader = DataLoader(dataset_, batch_size=batch_size, shuffle = True)","metadata":{"id":"LR64qTZ-f8YP","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:58.930682Z","iopub.execute_input":"2025-06-08T15:45:58.930868Z","iopub.status.idle":"2025-06-08T15:45:58.949129Z","shell.execute_reply.started":"2025-06-08T15:45:58.930852Z","shell.execute_reply":"2025-06-08T15:45:58.948504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_valid = TensorDataset(torch.tensor(input_ids_val), torch.tensor(labels_val),\n                          torch.tensor(attention_masks_val))\ndataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle = True)","metadata":{"id":"p_EFjQh5gS9F","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:58.949924Z","iopub.execute_input":"2025-06-08T15:45:58.950325Z","iopub.status.idle":"2025-06-08T15:45:58.955128Z","shell.execute_reply.started":"2025-06-08T15:45:58.950303Z","shell.execute_reply":"2025-06-08T15:45:58.954446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_test = TensorDataset(torch.tensor(input_ids_test), torch.tensor(labels_test),\n                          torch.tensor(attention_masks_test))\ndataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle = False)","metadata":{"id":"aKObaAVmgUql","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:58.955770Z","iopub.execute_input":"2025-06-08T15:45:58.955979Z","iopub.status.idle":"2025-06-08T15:45:58.970386Z","shell.execute_reply.started":"2025-06-08T15:45:58.955964Z","shell.execute_reply":"2025-06-08T15:45:58.969626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(dataset_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:58.971104Z","iopub.execute_input":"2025-06-08T15:45:58.971317Z","iopub.status.idle":"2025-06-08T15:45:58.989018Z","shell.execute_reply.started":"2025-06-08T15:45:58.971303Z","shell.execute_reply":"2025-06-08T15:45:58.988462Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BertBiLSTMClassifier","metadata":{"id":"HOdVKiqgW_KC"}},{"cell_type":"code","source":"class BertBiLSTMClassifier(nn.Module):\n    def __init__(self, num_classes, bert_frozen_layers=3, input_size=768, lstm_hidden_size=128):\n        # you can set bert_frozen_layers to an arbitrary number\n        super(BertBiLSTMClassifier, self).__init__()\n\n        # HeartBERT model with frozen layers\n        self.bert = AutoModel.from_pretrained(pretrained_model_name_or_path=\"/kaggle/input/pretrained_heartbert/transformers/pt20k_250/1/model\")\n\n        modules = [self.bert.embeddings, *self.bert.encoder.layer[:bert_frozen_layers]]\n        for module in modules:\n          for param in module.parameters():\n            param.requires_grad = False\n        # Bi-LSTM layer\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=lstm_hidden_size, bidirectional=True, batch_first=True)\n\n        # Fully connected layer for classification\n        self.fc = nn.Linear(lstm_hidden_size * 2, 1)\n        self.sigmoid = nn.Sigmoid()\n      #  self.softmax = nn.Softmax(dim=1)\n    \n\n    def forward(self, input_ids):\n        # BERT forward pass\n        bert_output = self.bert(input_ids)[0]\n        # Bi-LSTM forward pass\n        lstm_out, _ = self.lstm(bert_output)\n        # Use the last hidden state from Bi-LSTM\n        lstm_last_hidden_state = lstm_out[:, -1, :]\n        # Classification using fully connected layer\n        logit = self.fc(lstm_last_hidden_state)\n        res = self.sigmoid(logit)\n\n      #  res = self.softmax(logits)\n\n        return res.squeeze(-1)","metadata":{"id":"bNPHcnS-7_75","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:58.990785Z","iopub.execute_input":"2025-06-08T15:45:58.990976Z","iopub.status.idle":"2025-06-08T15:45:59.003702Z","shell.execute_reply.started":"2025-06-08T15:45:58.990956Z","shell.execute_reply":"2025-06-08T15:45:59.003001Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = BertBiLSTMClassifier(2)\nmodel = model.to(device)","metadata":{"id":"0fmIo3rS9HBn","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:45:59.004437Z","iopub.execute_input":"2025-06-08T15:45:59.004694Z","iopub.status.idle":"2025-06-08T15:45:59.312032Z","shell.execute_reply.started":"2025-06-08T15:45:59.004672Z","shell.execute_reply":"2025-06-08T15:45:59.311438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_params","metadata":{"id":"ZioFSxiHsHXS","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:46:12.227537Z","iopub.execute_input":"2025-06-08T15:46:12.227819Z","iopub.status.idle":"2025-06-08T15:46:12.233361Z","shell.execute_reply.started":"2025-06-08T15:46:12.227798Z","shell.execute_reply":"2025-06-08T15:46:12.232760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=5e-6) # setup the proper lr\n# lr=1e-6 40 epoch маловато, очень медленно, test loss 0.5638710278798552","metadata":{"id":"3NeBRPEiPTfA","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:46:12.521809Z","iopub.execute_input":"2025-06-08T15:46:12.522290Z","iopub.status.idle":"2025-06-08T15:46:12.526642Z","shell.execute_reply.started":"2025-06-08T15:46:12.522267Z","shell.execute_reply":"2025-06-08T15:46:12.525809Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Metrics","metadata":{"id":"WoldsV_pGJ_I"}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef compute_metrics(predictions, labels):\n    # Преобразуем в бинарные предсказания (если нужно)\n    predictions = (predictions > 0.5).cpu().numpy()\n    labels = labels.cpu().numpy()\n    \n    # Вычисление метрик\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=None, zero_division=0)\n    accuracy = accuracy_score(labels, predictions)\n\n    print(\"Class-wise Metrics:\")\n    for i in range(len(precision)):\n        print(f\"Class {i}: Precision={precision[i]:.4f}, Recall={recall[i]:.4f}, F1={f1[i]:.4f}\")\n\n    print(\"\\nAccuracy:\")\n    print(f\"Accuracy={accuracy:.4f}\")\n\ndef display_confusion_matrix(eval_pred, labels):\n    num_classes = 2\n\n    predictions = eval_pred.cpu().numpy()  # Перенос на CPU и преобразование в numpy\n    predictions = (predictions > 0.5).astype(int)  # Преобразование в 0 или 1\n\n    # predictions = np.argmax(eval_pred, axis=-1)\n    # predictions = eval_pred.cpu().numpy()\n    # predictions = (predictions > 0.5).cpu().numpy()\n    print(f\"Length of predictions: {len(predictions)}\")\n    print(f\"Length of labels: {len(labels)}\")\n\n    cm = confusion_matrix(labels, predictions)\n\n    # Plot confusion matrix\n    plt.figure(figsize=(num_classes*5, num_classes*5))\n\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"RdPu\", cbar=False,\n                xticklabels=np.arange(1, num_classes+1),\n                yticklabels=np.arange(1, num_classes+1))\n\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:46:15.080672Z","iopub.execute_input":"2025-06-08T15:46:15.081374Z","iopub.status.idle":"2025-06-08T15:46:15.350392Z","shell.execute_reply.started":"2025-06-08T15:46:15.081334Z","shell.execute_reply":"2025-06-08T15:46:15.349777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n\n# def compute_metrics(eval_pred, labels):\n\n#     predictions = np.argmax(eval_pred, axis=-1)\n\n#     # Compute precision, recall, F1, and accuracy\n#     precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=None)\n#     accuracy = accuracy_score(labels, predictions)\n\n#     # Compute micro and macro averages\n#     micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(labels, predictions, average='micro')\n#     macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n\n#     print(\"Class-wise Metrics:\")\n#     for i in range(len(precision)):\n#         print(f\"Class {i+1}: Precision={precision[i]:.4f}, Recall={recall[i]:.4f}, F1={f1[i]:.4f}\")\n\n#     print(\"\\nMicro Average Metrics:\")\n#     print(f\"Precision={micro_precision:.4f}, Recall={micro_recall:.4f}, F1={micro_f1:.4f}\")\n\n#     print(\"\\nMacro Average Metrics:\")\n#     print(f\"Precision={macro_precision:.4f}, Recall={macro_recall:.4f}, F1={macro_f1:.4f}\")\n\n#     print(\"\\nAccuracy:\")\n#     print(f\"Accuracy={accuracy:.4f}\")\n\n# def display_confusion_matrix(eval_pred, labels):\n#     num_classes = 2\n\n#     predictions = np.argmax(eval_pred, axis=-1)\n\n#     cm = confusion_matrix(labels, predictions)\n\n#     # Plot confusion matrix\n#     plt.figure(figsize=(num_classes, num_classes))\n\n#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n#                 xticklabels=np.arange(1, num_classes+1),\n#                 yticklabels=np.arange(1, num_classes+1))\n\n#     plt.xlabel('Predicted')\n#     plt.ylabel('Actual')\n#     plt.title('Confusion Matrix')\n#     plt.show()\n","metadata":{"id":"tclmuyY8aofz","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:46:17.316853Z","iopub.execute_input":"2025-06-08T15:46:17.317324Z","iopub.status.idle":"2025-06-08T15:46:17.321171Z","shell.execute_reply.started":"2025-06-08T15:46:17.317300Z","shell.execute_reply":"2025-06-08T15:46:17.320410Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{"id":"5JFeAu-0n1g3"}},{"cell_type":"code","source":"loss_epoch_train = []\nloss_epoch_valid = []\nnum_epochs = 20","metadata":{"id":"sjov9JAXPeEA","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:46:27.195376Z","iopub.execute_input":"2025-06-08T15:46:27.196190Z","iopub.status.idle":"2025-06-08T15:46:27.199425Z","shell.execute_reply.started":"2025-06-08T15:46:27.196135Z","shell.execute_reply":"2025-06-08T15:46:27.198729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from numpy import mean\ntotal_train_acc = np.zeros((num_epochs,))\ntotal_valid_acc = np.zeros((num_epochs,))\nfor epoch in range(num_epochs):\n  loss_batch_train = []\n  correct_train = []\n  correct_valid = []\n  accuracy_train = []\n  model.train()\n  for batch in dataloader:\n\n    inputs, labels, _ = batch\n\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n    outputs = model(inputs)\n    # loss = criterion(outputs.to(device), labels.type(torch.LongTensor).to(device))\n    labels = labels.to(device).float()  \n    loss = criterion(outputs, labels)\n    # predicted = torch.argmax(outputs.data, dim=1)\n    predicted = (outputs > 0.5).long()\n    correct_train.extend(predicted.eq(labels.to(device).data).float())\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    loss_batch_train.append(loss.item())\n\n  # if (epoch%10==0):\n  #   torch.save(model.state_dict(), '/content/drive/MyDrive/'+str(epoch)+'.pt')\n\n  total_train_acc[epoch] = torch.tensor(correct_train).mean().item()\n  loss_train_per_epoch = mean(loss_batch_train)\n  loss_epoch_train.append(loss_train_per_epoch)\n  print('train-loss',epoch,':', loss_epoch_train[-1])\n\n  model.eval()\n  loss_batch_valid = []\n\n  for batch in dataloader_valid:\n    inputs, labels, _ = batch\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n    outputs = model(inputs)\n    # loss = criterion(outputs.to(device), labels.type(torch.LongTensor).to(device))\n    labels = labels.to(device).float()  \n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    # predicted = torch.argmax(outputs.data, dim=1)\n    predicted = (outputs > 0.5).long()\n    correct_valid.extend(predicted.eq(labels.to(device).data).float())\n    loss_batch_valid.append(loss.item())\n\n  total_valid_acc[epoch] = torch.tensor(correct_valid).mean().item()\n  loss_valid_per_epoch = mean(loss_batch_valid)\n  loss_epoch_valid.append(loss_valid_per_epoch)\n  print(total_train_acc,total_valid_acc)\n  print('valid-loss',epoch,':',loss_epoch_valid[-1])\n  print('**********')\n","metadata":{"id":"GlBLq8OhkAuC","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:46:27.809003Z","iopub.execute_input":"2025-06-08T15:46:27.809295Z","iopub.status.idle":"2025-06-08T15:49:09.366564Z","shell.execute_reply.started":"2025-06-08T15:46:27.809271Z","shell.execute_reply":"2025-06-08T15:49:09.365741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(total_train_acc,total_valid_acc)","metadata":{"id":"fNrT4XQJ0pTJ","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:49:31.906808Z","iopub.execute_input":"2025-06-08T15:49:31.907370Z","iopub.status.idle":"2025-06-08T15:49:31.911244Z","shell.execute_reply.started":"2025-06-08T15:49:31.907344Z","shell.execute_reply":"2025-06-08T15:49:31.910491Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plot","metadata":{"id":"VX8bpLvQztZ1"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.rcParams.update({\n    'font.family': 'Times New Roman',\n    'font.size': 16,\n    'axes.titlesize': 16,\n    'axes.labelsize': 16,\n    'xtick.labelsize': 16,\n    'ytick.labelsize': 16,\n})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(loss_epoch_train,'r')\nplt.plot(loss_epoch_valid,'b')\nplt.title('train-loss vs eval-loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['loss_train','loss_eval']);","metadata":{"id":"pXNF9gthzs6k","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:49:33.347764Z","iopub.execute_input":"2025-06-08T15:49:33.348356Z","iopub.status.idle":"2025-06-08T15:49:33.612436Z","shell.execute_reply.started":"2025-06-08T15:49:33.348332Z","shell.execute_reply":"2025-06-08T15:49:33.611672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(total_train_acc, 'r')\nplt.plot(total_valid_acc, 'b')\nplt.legend(['total_train_acc', 'total_valid_acc'])","metadata":{"id":"EzoTirhWLcBN","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:49:39.568472Z","iopub.execute_input":"2025-06-08T15:49:39.568952Z","iopub.status.idle":"2025-06-08T15:49:39.733232Z","shell.execute_reply.started":"2025-06-08T15:49:39.568929Z","shell.execute_reply":"2025-06-08T15:49:39.732531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model,'/kaggle/working/model250.pt')\ntorch.save(model,'/kaggle/working/model2501.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:49:40.176455Z","iopub.execute_input":"2025-06-08T15:49:40.176994Z","iopub.status.idle":"2025-06-08T15:49:41.262106Z","shell.execute_reply.started":"2025-06-08T15:49:40.176965Z","shell.execute_reply":"2025-06-08T15:49:41.261311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/state_dict.pt')\ntorch.save(model.state_dict(), '/kaggle/working/state_dict1.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:49:44.132493Z","iopub.execute_input":"2025-06-08T15:49:44.133202Z","iopub.status.idle":"2025-06-08T15:49:45.200295Z","shell.execute_reply.started":"2025-06-08T15:49:44.133170Z","shell.execute_reply":"2025-06-08T15:49:45.199508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model_scripted = torch.jit.script(model) \n# model_scripted.save('/kaggle/working/model_scripted.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:49:45.201497Z","iopub.execute_input":"2025-06-08T15:49:45.202058Z","iopub.status.idle":"2025-06-08T15:49:45.204972Z","shell.execute_reply.started":"2025-06-08T15:49:45.202029Z","shell.execute_reply":"2025-06-08T15:49:45.204270Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test","metadata":{"id":"GwYU7iwav0p1"}},{"cell_type":"code","source":"model.eval()\nloss_batch_test = []\n\n# outputs_temp=torch.zeros((1,outputs.size()[1])).to(device)\noutputs_temp=torch.zeros((1)).to(device)\nlabels_temp = torch.zeros((1,))\n\nfor batch in dataloader_test:\n  with torch.no_grad():\n    inputs, labels, _ = batch\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n    # attention = attention.type(torch.IntTensor).to(device)\n    outputs = model(inputs)\n    #predicted = torch.argmax(outputs.data)\n    predicted = (outputs > 0.5).long()\n    correct_train.extend(predicted.eq(labels.to(device).data).float())\n\n  outputs_temp = torch.cat((outputs_temp,outputs),dim=0)\n  labels_temp = torch.cat((labels_temp,labels.to('cpu')),dim=0)\n\n  # loss = criterion(outputs.to(device), labels.type(torch.LongTensor).to(device))\n  labels = labels.to(device).float()  \n  outputs = model(inputs)\n  loss = criterion(outputs, labels)\n  loss_batch_test.append(loss.item())\n\nprint('test-loss: ', np.mean(loss_batch_test))","metadata":{"id":"DvRZWk0DpxSq","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:49:46.737980Z","iopub.execute_input":"2025-06-08T15:49:46.738285Z","iopub.status.idle":"2025-06-08T15:49:52.555902Z","shell.execute_reply.started":"2025-06-08T15:49:46.738262Z","shell.execute_reply":"2025-06-08T15:49:52.555295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"outputs_temp shape:\", outputs_temp.shape)\nprint(\"unique predictions:\", np.unique(np.argmax(outputs_temp.cpu().numpy(), axis=-1)))\nprint(\"unique labels:\", np.unique(labels_temp.cpu().numpy()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:49:52.557108Z","iopub.execute_input":"2025-06-08T15:49:52.557867Z","iopub.status.idle":"2025-06-08T15:49:52.562715Z","shell.execute_reply.started":"2025-06-08T15:49:52.557846Z","shell.execute_reply":"2025-06-08T15:49:52.562170Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# compute_metrics(outputs_temp[1:].to('cpu'),labels_temp[1:].to('cpu'))\ncompute_metrics(outputs_temp[1:].to('cpu'),labels_temp[1:].to('cpu'))","metadata":{"id":"XU68XuyKLXjo","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:49:52.563244Z","iopub.execute_input":"2025-06-08T15:49:52.563404Z","iopub.status.idle":"2025-06-08T15:49:56.137706Z","shell.execute_reply.started":"2025-06-08T15:49:52.563391Z","shell.execute_reply":"2025-06-08T15:49:56.136874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_confusion_matrix(outputs_temp[1:].to('cpu'),labels_temp[1:].to('cpu'))","metadata":{"id":"-5cG7fZxp4BZ","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:49:56.139236Z","iopub.execute_input":"2025-06-08T15:49:56.139461Z","iopub.status.idle":"2025-06-08T15:49:56.237017Z","shell.execute_reply.started":"2025-06-08T15:49:56.139444Z","shell.execute_reply":"2025-06-08T15:49:56.236476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T11:46:16.631778Z","iopub.status.idle":"2025-06-02T11:46:16.632066Z","shell.execute_reply.started":"2025-06-02T11:46:16.631909Z","shell.execute_reply":"2025-06-02T11:46:16.631925Z"}},"outputs":[],"execution_count":null}]}